{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TvyemfDlDmu"
   },
   "source": [
    "### Coursework coding instructions (please also see full coursework spec)\n",
    "\n",
    "Please choose if you want to do either Task 1 or Task 2. You should write your report about one task only.\n",
    "\n",
    "For the task you choose you will need to do two approaches:\n",
    "  - Approach 1, which can use use pre-trained embeddings / models\n",
    "  - Approach 2, which should not use any pre-trained embeddings or models\n",
    "We should be able to run both approaches from the same colab file\n",
    "\n",
    "#### Running your code:\n",
    "  - Your models should run automatically when running your colab file without further intervention\n",
    "  - For each task you should automatically output the performance of both models\n",
    "  - Your code should automatically download any libraries required\n",
    "\n",
    "#### Structure of your code:\n",
    "  - You are expected to use the 'train', 'eval' and 'model_performance' functions, although you may edit these as required\n",
    "  - Otherwise there are no restrictions on what you can do in your code\n",
    "\n",
    "#### Documentation:\n",
    "  - You are expected to produce a .README file summarising how you have approached both tasks\n",
    "\n",
    "#### Reproducibility:\n",
    "  - Your .README file should explain how to replicate the different experiments mentioned in your report\n",
    "\n",
    "Good luck! We are really looking forward to seeing your reports and your model code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LRWFk-kelDoA",
    "outputId": "a85868fc-4e43-41b9-c9b1-9677a35b6a08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl (1.8MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8MB 14.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 27.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2MB 50.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=6fbc5432223a213b53a4dcd12c3e97fba63d53a26845d71f8c668bd36f68cb5a\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses, tokenizers, transformers\n",
      "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.2\n"
     ]
    }
   ],
   "source": [
    "# You will need to download any word embeddings required for your code, e.g.:\n",
    "\n",
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#!unzip glove.6B.zip\n",
    "!pip install transformers\n",
    "# For any packages that Colab does not provide auotmatically you will also need to install these below, e.g.:\n",
    "\n",
    "#! pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k_jkkPTtWZel",
    "outputId": "0c73286e-a0b1-47e1-a85e-f535184fbbfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WX9TqmK7lDoK"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import codecs\n",
    "import transformers\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "X09jt8VRlDoM"
   },
   "outputs": [],
   "source": [
    "# Setting random seed and device\n",
    "SEED = 1\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "AqhlzLl6lDoO"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('./drive/MyDrive/NLP_CW/train.csv')\n",
    "dev_df = pd.read_csv('./drive/MyDrive/NLP_CW/dev.csv')\n",
    "test_df = pd.read_csv('./drive/MyDrive/NLP_CW/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3RCmF7xulDoP"
   },
   "outputs": [],
   "source": [
    "# Number of epochs\n",
    "epochs = 10\n",
    "\n",
    "# Proportion of training data for train compared to dev\n",
    "train_proportion = 0.8\n",
    "\n",
    "# batch_size\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qAgZW6K1lDoR"
   },
   "outputs": [],
   "source": [
    "# We define our training loop\n",
    "def train(train_iter, dev_iter, model, number_epoch, method=None):\n",
    "    \"\"\"\n",
    "    Training loop for the model, which calls on eval to evaluate after each epoch\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    print(\"Training model.\")\n",
    "    if method == 'bert':\n",
    "        train_loss = []\n",
    "        eval_loss = []\n",
    "\n",
    "        for epoch in range(1, number_epoch+1):\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            epoch_sse = 0\n",
    "            no_observations = 0\n",
    "\n",
    "            for batch in train_iter:\n",
    "                feature, target, mask = batch\n",
    "                \n",
    "                feature, target, mask = feature.to(device), target.to(device), mask.to(device)\n",
    "                model.batch_size = target.shape[0]\n",
    "                no_observations += target.shape[0]\n",
    "                output = model(feature, attention_mask=mask, labels=target)\n",
    "                predictions = output.logits.squeeze(1)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss = loss_fn(predictions, target)\n",
    "\n",
    "                sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy())\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()*target.shape[0]\n",
    "                epoch_sse += sse\n",
    "            \n",
    "            valid_loss, valid_mse, __, __ = eval(dev_iter, model, method=method)\n",
    "\n",
    "            epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\n",
    "            \n",
    "            train_loss += [epoch_loss]\n",
    "            eval_loss += [valid_loss]\n",
    "            \n",
    "            print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train MSE: {epoch_mse:.2f} | Train RMSE: {epoch_mse**0.5:.2f} | \\\n",
    "            Val. Loss: {valid_loss:.2f} | Val. MSE: {valid_mse:.2f} |  Val. RMSE: {valid_mse**0.5:.2f} |')\n",
    "        torch.save(model.state_dict(), './drive/MyDrive/NLP_CW/bert.pth')\n",
    "        np.save('./drive/MyDrive/NLP_CW/train_loss.npy', train_loss)\n",
    "        np.save('./drive/MyDrive/NLP_CW/eval_loss.npy', eval_loss)\n",
    "    else:\n",
    "        for epoch in range(1, number_epoch+1):\n",
    "\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            epoch_sse = 0\n",
    "            no_observations = 0  # Observations used for training so far\n",
    "\n",
    "            for batch in train_iter:\n",
    "\n",
    "                feature, target = batch\n",
    "\n",
    "                feature, target = feature.to(device), target.to(device)\n",
    "\n",
    "            # for RNN:\n",
    "                model.batch_size = target.shape[0]\n",
    "                no_observations = no_observations + target.shape[0]\n",
    "                model.hidden = model.init_hidden()\n",
    "\n",
    "                predictions = model(feature).squeeze(1)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                loss = loss_fn(predictions, target)\n",
    "\n",
    "                sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy())\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()*target.shape[0]\n",
    "                epoch_sse += sse\n",
    "\n",
    "            valid_loss, valid_mse, __, __ = eval(dev_iter, model)\n",
    "\n",
    "            epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\n",
    "            print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train MSE: {epoch_mse:.2f} | Train RMSE: {epoch_mse**0.5:.2f} | \\\n",
    "            Val. Loss: {valid_loss:.2f} | Val. MSE: {valid_mse:.2f} |  Val. RMSE: {valid_mse**0.5:.2f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NzXeDgHmlDob"
   },
   "outputs": [],
   "source": [
    "# We evaluate performance on our dev set\n",
    "def eval(data_iter, model, method=None):\n",
    "    \"\"\"\n",
    "    Evaluating model performance on the dev set\n",
    "    \"\"\"\n",
    "    if method == 'bert':\n",
    "        model.eval()\n",
    "        epoch_loss = 0\n",
    "        epoch_sse = 0\n",
    "        pred_all = []\n",
    "        trg_all = []\n",
    "        no_observations = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in data_iter:\n",
    "                feature, target, mask = batch\n",
    "\n",
    "                feature, target, mask = feature.to(device), target.to(device), mask.to(device)\n",
    "\n",
    "            # for RNN:\n",
    "                model.batch_size = target.shape[0]\n",
    "                no_observations = no_observations + target.shape[0]\n",
    "                output = model(feature, attention_mask=mask, labels=target)\n",
    "                predictions = output.logits.squeeze(1)\n",
    "\n",
    "                loss = loss_fn(predictions, target)\n",
    "\n",
    "            # We get the mse\n",
    "                pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
    "                sse, __ = model_performance(pred, trg)\n",
    "\n",
    "                epoch_loss += loss.item()*target.shape[0]\n",
    "                epoch_sse += sse\n",
    "                pred_all.extend(pred)\n",
    "                trg_all.extend(trg)\n",
    "    \n",
    "    else:\n",
    "        model.eval()\n",
    "        epoch_loss = 0\n",
    "        epoch_sse = 0\n",
    "        pred_all = []\n",
    "        trg_all = []\n",
    "        no_observations = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in data_iter:\n",
    "                feature, target = batch\n",
    "\n",
    "                feature, target = feature.to(device), target.to(device)\n",
    "\n",
    "            # for RNN:\n",
    "                model.batch_size = target.shape[0]\n",
    "                no_observations = no_observations + target.shape[0]\n",
    "                model.hidden = model.init_hidden()\n",
    "\n",
    "                predictions = model(feature).squeeze(1)\n",
    "                loss = loss_fn(predictions, target)\n",
    "\n",
    "            # We get the mse\n",
    "                pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
    "                sse, __ = model_performance(pred, trg)\n",
    "\n",
    "                epoch_loss += loss.item()*target.shape[0]\n",
    "                epoch_sse += sse\n",
    "                pred_all.extend(pred)\n",
    "                trg_all.extend(trg)\n",
    "\n",
    "    return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2_22fHHElDog"
   },
   "outputs": [],
   "source": [
    "# How we print the model performance\n",
    "def model_performance(output, target, print_output=False):\n",
    "    \"\"\"\n",
    "    Returns SSE and MSE per batch (printing the MSE and the RMSE)\n",
    "    \"\"\"\n",
    "\n",
    "    sq_error = (output - target)**2\n",
    "\n",
    "    sse = np.sum(sq_error)\n",
    "    mse = np.mean(sq_error)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    if print_output:\n",
    "        print(f'| MSE: {mse:.2f} | RMSE: {rmse:.2f} |')\n",
    "\n",
    "    return sse, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LcxmqrKhlDoj"
   },
   "outputs": [],
   "source": [
    "def create_vocab(data):\n",
    "    \"\"\"\n",
    "    Creating a corpus of all the tokens used\n",
    "    \"\"\"\n",
    "    tokenized_corpus = [] # Let us put the tokenized corpus in a list\n",
    "\n",
    "    for sentence in data:\n",
    "\n",
    "        tokenized_sentence = []\n",
    "\n",
    "        for token in sentence.split(' '): # simplest split is\n",
    "\n",
    "            tokenized_sentence.append(token)\n",
    "\n",
    "        tokenized_corpus.append(tokenized_sentence)\n",
    "\n",
    "    # Create single list of all vocabulary\n",
    "    vocabulary = []  # Let us put all the tokens (mostly words) appearing in the vocabulary in a list\n",
    "\n",
    "    for sentence in tokenized_corpus:\n",
    "\n",
    "        for token in sentence:\n",
    "\n",
    "            if token not in vocabulary:\n",
    "\n",
    "                if True:\n",
    "                    vocabulary.append(token)\n",
    "\n",
    "    return vocabulary, tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "jzQ0KLXslDoq"
   },
   "outputs": [],
   "source": [
    "def collate_fn_padd(batch):\n",
    "    '''\n",
    "    We add padding to our minibatches and create tensors for our model\n",
    "    '''\n",
    "\n",
    "    batch_labels = [l for f, l in batch]\n",
    "    batch_features = [f for f, l in batch]\n",
    "\n",
    "    batch_features_len = [len(f) for f, l in batch]\n",
    "\n",
    "    seq_tensor = torch.zeros((len(batch), max(batch_features_len))).long()\n",
    "\n",
    "    for idx, (seq, seqlen) in enumerate(zip(batch_features, batch_features_len)):\n",
    "        seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "\n",
    "    batch_labels = torch.FloatTensor(batch_labels)\n",
    "\n",
    "    return seq_tensor, batch_labels\n",
    "\n",
    "class Task1DatasetBert(Dataset):\n",
    "\n",
    "    def __init__(self, train_data, labels, mask):\n",
    "        self.x_train = train_data\n",
    "        self.y_train = labels\n",
    "        self.mask = mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.x_train[item], self.y_train[item], self.mask[item]\n",
    "\n",
    "class Task1Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, train_data, labels, mask):\n",
    "        self.x_train = train_data\n",
    "        self.y_train = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.x_train[item], self.y_train[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWaxTh2UlDoy"
   },
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, embedding, hidden_dim, vocab_size, batch_size, device):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2label = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers * num_directions, minibatch_size, hidden_dim)\n",
    "        return torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device), \\\n",
    "               torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embedded = self.embedding(sentence)\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embedded.view(len(embedded), self.batch_size, self.embedding_dim), self.hidden)\n",
    "\n",
    "        out = self.hidden2label(lstm_out[-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMKhSKhmEO4h"
   },
   "source": [
    "# Part 1 Second Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9paRh4jFI3_X",
    "outputId": "7c9a0402-c17f-41ff-b4d9-157b3c92a058"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setence tokenized and padded.\n",
      "data loader created.\n",
      "setence tokenized and padded.\n",
      "data loader created.\n",
      "setence tokenized and padded.\n",
      "data loader created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n",
      "| Epoch: 01 | Train Loss: 0.38 | Train MSE: 0.38 | Train RMSE: 0.61 |             Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.58 |\n",
      "| Epoch: 02 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |             Val. Loss: 0.36 | Val. MSE: 0.36 |  Val. RMSE: 0.60 |\n",
      "| Epoch: 03 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |             Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.58 |\n",
      "| Epoch: 04 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |             Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.58 |\n",
      "| Epoch: 05 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |             Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.58 |\n",
      "| Epoch: 06 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |             Val. Loss: 0.36 | Val. MSE: 0.36 |  Val. RMSE: 0.60 |\n",
      "| Epoch: 07 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |             Val. Loss: 0.37 | Val. MSE: 0.37 |  Val. RMSE: 0.61 |\n",
      "| Epoch: 08 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |             Val. Loss: 0.37 | Val. MSE: 0.37 |  Val. RMSE: 0.61 |\n",
      "| Epoch: 09 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |             Val. Loss: 0.36 | Val. MSE: 0.36 |  Val. RMSE: 0.60 |\n",
      "| Epoch: 10 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |             Val. Loss: 0.36 | Val. MSE: 0.36 |  Val. RMSE: 0.60 |\n"
     ]
    }
   ],
   "source": [
    "MaxLen = 40\n",
    "\n",
    "\n",
    "def create_dataloader(df):\n",
    "    data = df['original']\n",
    "    model_class, tokenizer_class, pretrained_weights = (transformers.BertModel, transformers.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "    model = model_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "    data_tokenized = data.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "    data_tokenized = pad_sequences(data_tokenized, maxlen=MaxLen, dtype='long', value=0, truncating='post')\n",
    "    print('setence tokenized and padded.')\n",
    "    attention_mask = []\n",
    "    for token in data_tokenized:\n",
    "        att_mask = [int(token_id > 0) for token_id in token]\n",
    "        attention_mask.append(att_mask)\n",
    "\n",
    "    data_tokenized = torch.tensor(data_tokenized)\n",
    "    label = torch.tensor(df['meanGrade'], dtype=torch.float32)\n",
    "    attention_mask = torch.tensor(attention_mask, dtype=torch.int32)\n",
    "    dataset = Task1DatasetBert(data_tokenized, label, attention_mask)\n",
    "    loader = torch.utils.data.DataLoader(dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "    print('data loader created.')\n",
    "    return loader\n",
    "\n",
    "train_loader = create_dataloader(train_df)\n",
    "dev_loader = create_dataloader(dev_df)\n",
    "test_loader = create_dataloader(test_df)\n",
    "\n",
    "model = transformers.BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                                   num_labels=1,\n",
    "                                                                   output_attentions=False,\n",
    "                                                                   output_hidden_states=False)\n",
    "model.hidden_dropout_prob = 0.3\n",
    "model.attention_dropout_prob = 0.3\n",
    "model.cuda()\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "loss_fn = loss_fn.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5, weight_decay=0.03)\n",
    "\n",
    "train(train_loader, dev_loader, model, epochs, 'bert')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E1o2OF5W4Ynh",
    "outputId": "ce8f13c6-8a47-41a7-bf29-fb48854e3524"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3556553167956216 0.35565531553414764\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_mse, __, __ = eval(test_loader, model, 'bert')\n",
    "print(test_loss, test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "1X15ozkg0Ts1",
    "outputId": "37df3f4b-c7cf-48c4-9b32-624977c21d67"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# draw training curve\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "train_loss = np.load('./drive/MyDrive/NLP_CW/train_loss.npy')\n",
    "eval_loss = np.load('./drive/MyDrive/NLP_CW/eval_loss.npy')\n",
    "x = list(range(len(train_loss)))\n",
    "plt.plot(x, train_loss)\n",
    "plt.plot(x, eval_loss)\n",
    "plt.legend(['train loss', 'eval loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdeFaoc3lDpK"
   },
   "source": [
    "#### Approach 2: No pre-trained representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46gm47T4lDpQ",
    "outputId": "6675cde2-9bba-4955-a292-b78cc0f3e12c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train performance:\n",
      "| MSE: 0.13 | RMSE: 0.37 |\n",
      "\n",
      "Dev performance:\n",
      "| MSE: 0.36 | RMSE: 0.60 |\n"
     ]
    }
   ],
   "source": [
    "train_and_dev = train_df['edit']\n",
    "\n",
    "training_data, dev_data, training_y, dev_y = train_test_split(train_df['edit'], train_df['meanGrade'],\n",
    "                                                                        test_size=(1-train_proportion),\n",
    "                                                                        random_state=42)\n",
    "\n",
    "# We train a Tf-idf model\n",
    "count_vect = CountVectorizer(stop_words='english')\n",
    "train_counts = count_vect.fit_transform(training_data)\n",
    "transformer = TfidfTransformer().fit(train_counts)\n",
    "train_counts = transformer.transform(train_counts)\n",
    "regression_model = LinearRegression().fit(train_counts, training_y)\n",
    "\n",
    "# Train predictions\n",
    "predicted_train = regression_model.predict(train_counts)\n",
    "\n",
    "# Calculate Tf-idf using train and dev, and validate model on dev:\n",
    "test_and_test_counts = count_vect.transform(train_and_dev)\n",
    "transformer = TfidfTransformer().fit(test_and_test_counts)\n",
    "\n",
    "test_counts = count_vect.transform(dev_data)\n",
    "\n",
    "test_counts = transformer.transform(test_counts)\n",
    "\n",
    "# Dev predictions\n",
    "predicted = regression_model.predict(test_counts)\n",
    "\n",
    "# We run the evaluation:\n",
    "print(\"\\nTrain performance:\")\n",
    "sse, mse = model_performance(predicted_train, training_y, True)\n",
    "\n",
    "print(\"\\nDev performance:\")\n",
    "sse, mse = model_performance(predicted, dev_y, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HyHwHkUlDpa"
   },
   "source": [
    "#### Baseline for task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7DA3q4o1lDpd",
    "outputId": "fa88780f-7ed0-4c2b-f2cf-bc0b2883626d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline performance:\n",
      "| MSE: 0.34 | RMSE: 0.58 |\n"
     ]
    }
   ],
   "source": [
    "# Baseline for the task\n",
    "pred_baseline = torch.zeros(len(dev_y)) + np.mean(training_y)\n",
    "print(\"\\nBaseline performance:\")\n",
    "sse, mse = model_performance(pred_baseline, dev_y, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84nQDZyBlDpg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "task_1_main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
